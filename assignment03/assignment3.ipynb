{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Linear Regression\n",
    "\n",
    "Only use the already imported library `numpy` and the Python standard library. Make sure that the datasets `dataLinReg2D.txt`, `dataQuadReg2D.txt`, `dataQuadReg2D_noisy.txt` and `airfoil_self_noise.dat` are in the same directory as the notebook.\n",
    "\n",
    "List your team members (name and immatriculation number) and indicate whether you are a B.Sc. Data Science or other group in the following cell:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kuang-Yu Li, st169971@stud.uni-stuttgart.de, 3440829\n",
    "- Ya Jen Hsu, st169013@stud.uni-stuttgart.de, 3449448\n",
    "- Gabriella Ilena, st169935@stud.uni-stuttgart.de, 3440942"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Ridge Regression for Polynomial 2D Functions\n",
    "\n",
    "Each line in the data sets consists of a data entry `(x,y)` with a 2D point `x` and a 1D function output `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load required packages and datasets. Do not modify.\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_dataset(path):\n",
    "    data = np.loadtxt(path)\n",
    "    X, y = data[:, :2], data[:, 2]\n",
    "    \n",
    "    return X, y\n",
    "    \n",
    "\n",
    "X_lin, y_lin = load_dataset(\"dataLinReg2D.txt\")\n",
    "X_quad, y_quad = load_dataset(\"dataQuadReg2D.txt\")\n",
    "X_noise, y_noise = load_dataset(\"dataQuadReg2D_noisy.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Extend the `make_features` method to also compute quadratic features (`ftype = 'quad'`). You may also experiment with other feature transformations, e.g. third-order polynomials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.         0.974633  -0.792363 ]\n",
      " [ 1.         0.322351   1.8034   ]\n",
      " [ 1.         1.35589    0.389593 ]\n",
      " [ 1.         0.0859725 -1.0693   ]\n",
      " [ 1.        -0.6436    -0.400372 ]\n",
      " [ 1.        -0.88582    0.602299 ]\n",
      " [ 1.        -0.05727   -0.68016  ]\n",
      " [ 1.         1.3165    -0.570268 ]\n",
      " [ 1.        -1.2269     0.0372067]\n",
      " [ 1.        -0.307733  -0.217767 ]\n",
      " [ 1.         2.01961   -0.143457 ]\n",
      " [ 1.        -0.346529  -0.474998 ]\n",
      " [ 1.         0.624072  -0.437087 ]\n",
      " [ 1.         1.50335    1.33349  ]\n",
      " [ 1.         1.00832    0.019365 ]\n",
      " [ 1.         2.61706    1.87667  ]\n",
      " [ 1.        -2.26652   -0.0321358]\n",
      " [ 1.         1.30894    2.11607  ]\n",
      " [ 1.         1.09092    1.04312  ]\n",
      " [ 1.         1.20046    0.469415 ]\n",
      " [ 1.         0.780383   0.0111708]\n",
      " [ 1.        -0.10054   -1.40242  ]\n",
      " [ 1.         1.21726    1.06692  ]\n",
      " [ 1.         2.33033    0.0444168]\n",
      " [ 1.         0.4038     1.44355  ]\n",
      " [ 1.        -1.35011   -0.320975 ]\n",
      " [ 1.        -1.52264   -0.050186 ]\n",
      " [ 1.        -0.61236   -0.782373 ]\n",
      " [ 1.         0.0934571  0.36143  ]\n",
      " [ 1.         0.602785  -0.128348 ]\n",
      " [ 1.         1.4923     0.705827 ]\n",
      " [ 1.        -0.973672  -0.578001 ]\n",
      " [ 1.        -0.0543634 -0.192952 ]\n",
      " [ 1.         0.0864153 -1.98039  ]\n",
      " [ 1.        -0.0877569  0.829806 ]\n",
      " [ 1.        -0.16916   -0.650094 ]\n",
      " [ 1.         0.190051   0.590431 ]\n",
      " [ 1.         2.24335   -0.540746 ]\n",
      " [ 1.         2.29005    1.75977  ]\n",
      " [ 1.         1.19283   -0.217893 ]\n",
      " [ 1.        -0.0130601 -0.775068 ]\n",
      " [ 1.         1.50782    0.26217  ]\n",
      " [ 1.        -0.495035   1.38466  ]\n",
      " [ 1.        -2.57561    0.0101092]\n",
      " [ 1.        -0.550798   0.793548 ]\n",
      " [ 1.         0.33286   -0.790275 ]\n",
      " [ 1.         0.600883   0.363704 ]\n",
      " [ 1.        -0.967231  -0.124756 ]\n",
      " [ 1.         0.86357    0.915796 ]\n",
      " [ 1.        -2.00479   -1.20705  ]\n",
      " [ 1.        -1.5744    -1.06133  ]\n",
      " [ 1.        -1.24246   -0.0532174]\n",
      " [ 1.        -0.142512   0.437945 ]\n",
      " [ 1.        -0.190588  -1.13174  ]\n",
      " [ 1.         0.0717578  0.657072 ]\n",
      " [ 1.        -0.847836   1.68174  ]\n",
      " [ 1.        -2.61233   -0.160418 ]\n",
      " [ 1.        -0.150264  -0.0907573]\n",
      " [ 1.         1.01469   -1.21428  ]\n",
      " [ 1.        -0.123068   0.730712 ]\n",
      " [ 1.         1.25154   -0.526393 ]\n",
      " [ 1.         0.572973  -0.0325019]\n",
      " [ 1.         0.744767   0.567598 ]\n",
      " [ 1.         0.767336   1.28563  ]\n",
      " [ 1.         0.27401    0.0259356]\n",
      " [ 1.         0.0387451  0.863472 ]\n",
      " [ 1.         1.37652    0.883576 ]\n",
      " [ 1.        -0.24544   -1.50491  ]\n",
      " [ 1.        -1.01056    0.463139 ]\n",
      " [ 1.        -0.266867  -0.7196   ]\n",
      " [ 1.        -0.698632   0.320259 ]\n",
      " [ 1.        -0.487443  -0.578519 ]\n",
      " [ 1.         0.111006  -2.07078  ]\n",
      " [ 1.        -0.688077  -2.78516  ]\n",
      " [ 1.        -1.5226     1.12646  ]\n",
      " [ 1.         0.717233   0.685768 ]\n",
      " [ 1.        -0.380492   0.59173  ]\n",
      " [ 1.         1.00786   -0.027513 ]\n",
      " [ 1.         0.376207   0.463145 ]\n",
      " [ 1.        -1.77862    0.336624 ]\n",
      " [ 1.        -0.669653   0.093832 ]\n",
      " [ 1.        -0.478987   1.15075  ]\n",
      " [ 1.        -0.275699  -0.811538 ]\n",
      " [ 1.        -0.185535   0.161834 ]\n",
      " [ 1.         2.39935   -0.560211 ]\n",
      " [ 1.         0.478475  -0.380631 ]\n",
      " [ 1.         0.0151748  0.413237 ]\n",
      " [ 1.         1.8743    -0.17471  ]\n",
      " [ 1.        -0.034751   0.165149 ]\n",
      " [ 1.         0.440518   0.61982  ]\n",
      " [ 1.         1.41156   -0.15232  ]\n",
      " [ 1.         2.04356    0.799842 ]\n",
      " [ 1.         1.96347    0.652255 ]\n",
      " [ 1.         0.63714    0.24976  ]\n",
      " [ 1.         0.189887  -0.417674 ]\n",
      " [ 1.        -1.3693     0.262116 ]\n",
      " [ 1.        -1.66794    1.34258  ]\n",
      " [ 1.         0.0747798  0.622185 ]\n",
      " [ 1.        -0.292759  -0.473951 ]\n",
      " [ 1.         0.230967  -0.255208 ]]\n"
     ]
    }
   ],
   "source": [
    "def make_features(X, ftype='lin'):\n",
    "    n, d = X.shape\n",
    "    \n",
    "    if ftype == 'lin': # Linear feature transformation (including intercept)\n",
    "        Phi = np.empty((n, d+1))\n",
    "        Phi[:, 0] = 1\n",
    "        Phi[:, 1:] = X\n",
    "\n",
    "    elif ftype == 'quad':  # Quadratic feature transformation\n",
    "        # Implement for task 1.1.\n",
    "        Phi = np.empty((n, d+1))\n",
    "        Phi[:, 0] = 1 # Intercept\n",
    "        Phi[:, ]\n",
    "\n",
    "        pass\n",
    "    \n",
    "    elif ftype == 'nasa':\n",
    "        # Implement for task 2.2.\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        raise Exception('Feature type {} not implemented yet'.format(ftype))\n",
    "    \n",
    "    return Phi\n",
    "\n",
    "# Testing\n",
    "Phi = make_features(X_lin, 'lin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Implement Ridge Regression to fit a polynomial function to the data sets with the regularization parameter `lambda_` and feature type `ftype`.\n",
    "\n",
    "Fill out the methods in `RidgeRegression` to train (`fit`) and predict (`predict`). Feel free to introduce new fields and methods based on your needs, but the methods `fit` and `predict` are required and their interface should not be changed. You need to store the vector of regression coefficients in the field `self.beta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement your solution here.\n",
    "class RidgeRegression(object):\n",
    "    def __init__(self, lambda_, ftype = 'lin'):\n",
    "        self.lambda_ = lambda_\n",
    "        self.ftype = ftype\n",
    "        self.beta = None  # Learned regression coefficients.\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X is an array of shape (n, d), \n",
    "            where n is the number of samples and d is the number of features.\n",
    "        y is an array of shape (n,)\n",
    "        \"\"\"\n",
    "        Phi = make_features(X, self.ftype)\n",
    "        \n",
    "        # Implement your solution here.\n",
    "        self.beta = np.zeros(Phi.shape[1])\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        X is an array with shape (n, d).\n",
    "        The method returns an array of shape (n,).\n",
    "        \"\"\"\n",
    "        Phi = make_features(X, self.ftype)\n",
    "        \n",
    "        # Implement your solution here.\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Implement the function `MSE` to compute the mean squared error. `y_pred` and `y_true` are the vectors of predicted and true function outputs respectively with shape `(n,)`, where `n` is the number of samples. The function returns a single float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y_pred, y_true):\n",
    "    # Implement your solution here.\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Evaluate your Ridge Regression model with linear features on the linear `(X_lin, y_lin)` data set. Try to find a good `lambda_`. How does it perform with quadratic features on this data set? Report the MSE on the full data set when trained on the full data set. (Ideally, repeat this for different Ridge regularization parameters `lambda_` and generate a nice bar plot of the MSE for various `lambda_`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(regression_model, X, y):\n",
    "    regression_model.fit(X, y)\n",
    "    yhat = regression_model.predict(X)\n",
    "    print('MSE:', MSE(yhat, y))\n",
    "\n",
    "    \n",
    "def plot_data_and_model(regression_model, X, y):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from mpl_toolkits.mplot3d.axes3d import Axes3D\n",
    "    %matplotlib notebook\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection = '3d')\n",
    "    ax.scatter(X[:, 0], X[:, 1], y, marker = 'o')\n",
    "    \n",
    "    xmin = X.min(0)\n",
    "    xmax = X.max(0)\n",
    "\n",
    "    x0grid, x1grid = np.mgrid[xmin[0]:xmax[0]:.3, xmin[1]:xmax[1]:.3]\n",
    "\n",
    "    xdim0, xdim1 = np.shape(x0grid)\n",
    "    xsize = np.size(x0grid)\n",
    "\n",
    "    x0hat = x0grid.flatten()\n",
    "    x1hat = x1grid.flatten()\n",
    "    x0hat = x0hat.reshape((np.size(x0hat), 1))\n",
    "    x1hat = x1hat.reshape((np.size(x1hat), 1))\n",
    "    xhat = np.append(x0hat, x1hat, 1)\n",
    "    xhatfv = make_features(xhat, regression_model.ftype)\n",
    "    yhat = xhatfv.dot(regression_model.beta)\n",
    "    ygrid = yhat.reshape((xdim0, xdim1))\n",
    "    ax.plot_wireframe(x0grid, x1grid, ygrid)\n",
    "    ax.auto_scale_xyz([xmin[0], xmax[0]], [xmin[1], xmax[1]], [y.min(), y.max()])\n",
    "    fig.show()\n",
    "\n",
    "    \n",
    "# Implement your solution here.\n",
    "regression_lin = RidgeRegression(lambda_=0.1, ftype='lin')\n",
    "train_evaluate(regression_lin, X_lin, y_lin)\n",
    "\n",
    "# If the plot doesn't show the first time, run this cell again\n",
    "plot_data_and_model(regression_lin, X_lin, y_lin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==> *Write your observations here and report your results.* (double klick here to edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Try to find a good model (including features and suitable `lambda_` parameters) for the quadratic data set `(X_quad, y_quad)`. Report the MSE on the full data set when trained on the full data set.  (Ideally, repeat this for different Ridge regularization parameters `lambda_` and generate a nice bar plot of the MSE for various `lambda_`.) Also plot your predicted model using the method above `plot_data_and_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement your model tests here using the quadratic data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==> *Write your observations here and report your results.* (double klick here to edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Try to find a good model (including features and suitable `lambda_` parameters) for the noisy data set `(X_noise, y_noise)`. Report the MSE on the full data set when trained on the full data set.  (Ideally, repeat this for different Ridge regularization parameters `lambda_` and generate a nice bar plot of the MSE for various `lambda_`.) Also plot your predicted model using the method above `plot_data_and_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement your model tests here using the noisy data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==> *Write your observations here and report your results.* (double klick here to edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Airfoil Self-Noise Prediction\n",
    "\n",
    "The air self-noise dataset is a [dataset by NASA](https://archive.ics.uci.edu/ml/datasets/Airfoil+Self-Noise). Your task is to find a good feature mapping for ridge regression to achieve the lowest possible prediction error.\n",
    "\n",
    "1) Explain the content of the dataset in few words. What are the input features? What is the output?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==> *Write your response here* (double klick here to edit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    data = np.genfromtxt(path)\n",
    "    X, y = data[:, :5], data[:, 5]\n",
    "    \n",
    "    return X, y\n",
    "    \n",
    "\n",
    "X, y = load_dataset('airfoil_self_noise.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Implement the option `ftype = 'nasa'` in the `make_features` function. You should try any type of non-linear features and interactions between features. You are not restricted to the feature mapping presented in the lecture. It is helpful to think about the domain and characteristics of the features, e.g. how do you work with periodic features.\n",
    "\n",
    "Explain the choice of your features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==> *Briefly explain your chosen features.* (double klick here to edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Evaluate your Ridge Regression model with your chosen features on the data set. Try to find a good `lambda_`. Report the MSE on the full data set when trained on the full data set. (Ideally, repeat this for different Ridge regularization parameters `lambda_` and generate a nice bar plot of the MSE for various `lambda_`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(regression_model, X, y):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2020)\n",
    "    \n",
    "    regression_model.fit(X_train, y_train)\n",
    "    yhat_test = regression_model.predict(X_test)\n",
    "    print('MSE:', MSE(yhat_test, y_test))\n",
    "\n",
    "your_regression = RidgeRegression(lambda_=0.1, ftype='nasa')\n",
    "train_evaluate(your_regression, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==> *Write your observations here and report your results.* (double klick here to edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For all students other than B.Sc. Data Science:**\n",
    "\n",
    "4) Implement the function `cross_validation` to evaluate the prediction error of your model. Report the mean squared error from cross-validation. (Ideally, repeat this for different Ridge regularization parameters `lambda_` and generate a nice bar plot of the MSE for various `lambda_`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(regression_model, X, y):\n",
    "    pass\n",
    "\n",
    "your_regression = RidgeRegression(lambda_=0.1, , ftype='nasa')\n",
    "cross_validation(your_regression, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==> *Write your observations here and report your results.* (double klick here to edit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}